{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch8vppwyJbFO",
        "outputId": "bc85f09f-506c-4bf9-f2cf-ca51b79e0cb9"
      },
      "outputs": [],
      "source": [
        "# !pip install swig\n",
        "# !pip install gymnasium \n",
        "# !pip install matplotlib\n",
        "# !pip install pygame\n",
        "# !pip install imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jhnSat8pIOIW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import random\n",
        "\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UPw0vSuHUqsE"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "TOTAL_STEPS = 1000000\n",
        "START_EPSILON = 1\n",
        "END_EPSILON = 0.05\n",
        "STEP_DECAY_FINAL_STEP = TOTAL_STEPS\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_GRAD_NORM = 5000\n",
        "\n",
        "GAMMA = 0.99\n",
        "\n",
        "TIMESTEPS_PER_EPOCH = 100\n",
        "BATCH_SIZE = 4096\n",
        "\n",
        "REFRESH_TARGET_NETWORK_FREQ = 500\n",
        "REPLAY_BUFFER_SIZE = 100000\n",
        "LOSS_SHOW_FREQ = 50000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZCS5lsWNVUaw"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "64XERVljJTBu"
      },
      "outputs": [],
      "source": [
        "def run_policy(env, agent):\n",
        "  frames = list()\n",
        "\n",
        "  state, info = env.reset()\n",
        "  for _ in range(1000):\n",
        "\n",
        "      qvalues = agent.get_qvalues([state])\n",
        "      action = agent.sample_actions(qvalues)[0]\n",
        "\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      frames.append(env.render())\n",
        "\n",
        "      if terminated or truncated:\n",
        "          observation, info = env.reset()\n",
        "          break\n",
        "\n",
        "\n",
        "  env.close()\n",
        "\n",
        "  return np.array(frames)\n",
        "\n",
        "\n",
        "def update_animation_canvas(num, frames, animation_canvas):\n",
        "    animation_canvas.set_data(frames[num])\n",
        "    return animation_canvas\n",
        "\n",
        "\n",
        "def animate_policy_actions(env, policy=None):\n",
        "  frames = run_policy(env, policy)\n",
        "\n",
        "  fig = plt.figure()\n",
        "  animation_canvas = plt.imshow(frames[0])\n",
        "\n",
        "  plt.title(\"Policy Acting Animation\")\n",
        "\n",
        "\n",
        "  frames_ani = animation.FuncAnimation(fig, update_animation_canvas, frames=len(frames), fargs=(frames, animation_canvas), interval=100)\n",
        "  plt.close()\n",
        "\n",
        "  return frames_ani\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pekcrrX9ITmv"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(torch.nn.Module):\n",
        "  def __init__(self, state_dim, n_actions, epsilon=0):\n",
        "    super(DQNAgent, self).__init__()\n",
        "\n",
        "    self.epsilon = epsilon\n",
        "    self.n_actions = n_actions\n",
        "    self.state_dim = state_dim\n",
        "\n",
        "\n",
        "    self.qvalue_network_estimator = torch.nn.ModuleList()\n",
        "\n",
        "    self.qvalue_network_estimator.append(torch.nn.Linear(self.state_dim, 192))\n",
        "    self.qvalue_network_estimator.append(torch.nn.ReLU())\n",
        "    self.qvalue_network_estimator.append(torch.nn.Linear(192, 256))\n",
        "    self.qvalue_network_estimator.append(torch.nn.ReLU())\n",
        "    self.qvalue_network_estimator.append(torch.nn.Linear(256, 64))\n",
        "    self.qvalue_network_estimator.append(torch.nn.ReLU())\n",
        "    self.qvalue_network_estimator.append(torch.nn.Linear(64, self.n_actions))\n",
        "\n",
        "  def forward(self, state):\n",
        "    q_values = state\n",
        "    for layer in self.qvalue_network_estimator:\n",
        "      q_values = layer(q_values)\n",
        "    return q_values\n",
        "\n",
        "  def get_qvalues(self, states):\n",
        "    states = torch.tensor(states, device=device, dtype=torch.float32)\n",
        "    q_values = self.forward(states)\n",
        "\n",
        "    return q_values.data.cpu().numpy()\n",
        "\n",
        "  def sample_actions(self, q_values):\n",
        "    batch_size, n_actions = q_values.shape\n",
        "    random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "    best_actions = q_values.argmax(axis=-1)\n",
        "    should_explore = np.random.choice([0, 1], batch_size, p=[1-self.epsilon, self.epsilon])\n",
        "\n",
        "    return np.where(should_explore, random_actions, best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EWE2dtw0U5m7"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, size):\n",
        "    self.size = size\n",
        "    self.buffer = list()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    item = (state, action, reward, next_state, done)\n",
        "\n",
        "    if len(self.buffer)<self.size:\n",
        "      self.buffer.append(item)\n",
        "    else:\n",
        "      self.buffer[random.randint(0, self.size-1)] = item\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    idxs = np.random.choice(len(self.buffer), batch_size)\n",
        "    samples = [self.buffer[i] for i in idxs]\n",
        "    states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(done_flags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kv3hkrGPYSbK"
      },
      "outputs": [],
      "source": [
        "def run_and_record(start_state, agent, env, exp_replay, n_steps=1):\n",
        "  s = start_state\n",
        "  sum_rewards = 0\n",
        "\n",
        "  for _ in range(n_steps):\n",
        "    q_values = agent.get_qvalues([s])\n",
        "    a = agent.sample_actions(q_values)[0]\n",
        "    next_s, r, terminated, truncated, _ = env.step(a) ######## done dimensions\n",
        "    done = terminated or truncated\n",
        "    sum_rewards+=r\n",
        "    exp_replay.add(s, a, r, next_s, done)\n",
        "    if done:\n",
        "      s, _ = env.reset()\n",
        "    else:\n",
        "      s = next_s\n",
        "\n",
        "  return sum_rewards, s\n",
        "\n",
        "\n",
        "def compute_td_loss(agent, target_network, states, actions, rewards, next_states, done_flags, gamma=0.99, device=device):\n",
        "  states = torch.tensor(states, device=device, dtype=torch.float32)\n",
        "  actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
        "  rewards = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
        "  next_states = torch.tensor(next_states, device=device, dtype=torch.float32)\n",
        "  done_flags = torch.tensor(done_flags.astype(\"float32\"), device=device, dtype=torch.float32)\n",
        "\n",
        "  predicted_qvalues = agent(states)\n",
        "  predicted_next_qvalues = target_network(next_states)\n",
        "  predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions] ##### check for :\n",
        "\n",
        "  next_state_values, _ = torch.max(predicted_next_qvalues, dim=1)\n",
        "  target_qvalues_for_actions = rewards + gamma*next_state_values*(1-done_flags)\n",
        "  loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach())**2) ### double check\n",
        "\n",
        "  return loss\n",
        "\n",
        "def epsilon_schedule(start_eps, end_eps, step, final_step):\n",
        "  return start_eps + (end_eps - start_eps)*min(step, final_step)/final_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aZN_SQWtSrr4"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(state_dim, n_actions, epsilon=0.5).to(device)\n",
        "target_network = DQNAgent(state_dim, n_actions, epsilon=0.5).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())\n",
        "\n",
        "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "\n",
        "opt = torch.optim.Adam(agent.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "7dhtVEzienVl",
        "outputId": "2c8b40e3-538d-4875-af7e-71a9c5490a85"
      },
      "outputs": [],
      "source": [
        "frames_animation = animate_policy_actions(env, agent)\n",
        "#HTML(frames_animation.to_html5_video())\n",
        "frames_animation.save(\"animation_before_training.gif\", writer=\"pillow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DyloTYDdTFS",
        "outputId": "1de26949-91de-4f58-d7a7-9f8840312214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0, loss: 104.813995\n",
            "Step: 50000, loss: 29.936726\n",
            "Step: 100000, loss: 20.897337\n",
            "Step: 150000, loss: 15.799156\n",
            "Step: 200000, loss: 9.514006\n",
            "Step: 250000, loss: 50.602657\n",
            "Step: 300000, loss: 75.415100\n",
            "Step: 350000, loss: 31.569639\n",
            "Step: 400000, loss: 42.221207\n",
            "Step: 450000, loss: 20.224836\n",
            "Step: 500000, loss: 14.642189\n",
            "Step: 550000, loss: 16.187210\n",
            "Step: 600000, loss: 18.033875\n",
            "Step: 650000, loss: 15.197511\n",
            "Step: 700000, loss: 8.487288\n"
          ]
        }
      ],
      "source": [
        "state, _ = env.reset()\n",
        "\n",
        "for step in range(TOTAL_STEPS):\n",
        "  opt.zero_grad()\n",
        "\n",
        "  agent.epsilon = epsilon_schedule(START_EPSILON, END_EPSILON, step, STEP_DECAY_FINAL_STEP)\n",
        "  _, state = run_and_record(state, agent, env, exp_replay, TIMESTEPS_PER_EPOCH)\n",
        "\n",
        "  states, actions, rewards, next_states, done_flags = exp_replay.sample(BATCH_SIZE)\n",
        "\n",
        "  loss = compute_td_loss(agent, target_network, states, actions, rewards, next_states, done_flags, gamma=GAMMA, device=device)\n",
        "  loss.backward()\n",
        "  grad_norm = torch.nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "  opt.step()\n",
        "\n",
        "  if step%LOSS_SHOW_FREQ == 0:\n",
        "    print(f\"Step: {step}, loss: {loss.data.cpu().item():.6f}\")\n",
        "  if step%REFRESH_TARGET_NETWORK_FREQ == 0:\n",
        "    target_network.load_state_dict(agent.state_dict())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2sqS89ZYNU0"
      },
      "outputs": [],
      "source": [
        "frames_animation = animate_policy_actions(env, agent)\n",
        "torch.save(agent.state_dict(), \"trained_agent.pth\")\n",
        "#HTML(frames_animation.to_html5_video())\n",
        "frames_animation.save(\"animation_after_training.gif\", writer=\"pillow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "140J6wNejsQi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
